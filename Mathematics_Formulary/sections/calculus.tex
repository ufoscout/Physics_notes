\chapter{Calculus}
\textbf{Definition:} Calculus (from Latin calculus, literally 'small pebble', used for counting and calculations, as on an abacus) is the mathematical study of continuous change. It has two major branches:
\begin{itemize}  
	\item \textbf{differential calculus:} concerning rates of change and slopes of curves;
	\item \textbf{integral calculus:} concerning accumulation of quantities and the areas under and between curves
\end{itemize}
These two branches are related to each other by the fundamental theorem of calculus. 


\section{Limits}
\subsection{Limits Rules}
If $ \lim_{x \rightarrow c }f(x) = L_1 $ and $ \lim_{x \rightarrow c } g(x) = L_2 $ then:

\begin{itemize}
	\item $ \displaystyle \lim_{x \rightarrow c} \left[ f(x) \pm g(x) \right] = L_1 \pm L_2 $
	\item $ \displaystyle \lim_{x \rightarrow c} \left[ f(x) \cdot g(x) \right] = L_1 \cdot L_2 $
	\item $ \displaystyle \lim_{x \rightarrow c} \frac{f(x)}{g(x)} = \frac{L_1}{L_2} $ \ \ \ \  if $L_2 \ne 0$
	\item $ \displaystyle \lim_{x \rightarrow c} f(x)^n = L_1^n $ \ \ \ \  if $n$ is a positive integer
	\item $ \displaystyle \lim_{x \rightarrow c} f(x)^{\frac{1}{n}} = L_1^{\frac{1}{n}} $ \ \ \ \  if $n$ is a positive integer, and, if $n$ is even, then $L_1>0$
	\item $ \displaystyle \lim_{x \rightarrow c} \frac{f(x)}{g(x)} = \lim_{x \rightarrow c} \frac{f^\prime(x)}{g^\prime(x)}$ \ \ \ \ if $ \lim_{x \rightarrow c } f(x) = \lim_{x \rightarrow c } g(x) = L $ \ \ (L'HÃ´pital's rule)
\end{itemize}

\subsubsection{Substitution rule}
 $ \displaystyle 
\text{Given  } \lim_{x\rightarrow c}f(x)=l \text{   then   } \lim_{x\rightarrow c}g(f(x))=\lim_{y\rightarrow l}g(y) 
$

\subsection{Common Limits}
\subsubsection{Simple functions}

\begin{tabularx}{\textwidth}{ l l }
 $ \displaystyle \lim_{x \rightarrow c} a = a $ & 
 $ \displaystyle \lim_{x \rightarrow c} x = c $ \\
  $ \displaystyle \lim_{x \rightarrow c} (ax + b) = ac+b $ &
  $ \displaystyle \lim_{x \rightarrow c} x^r = c^r $ \ \ \ if $r$ is a positive integer \\
 
 $ \displaystyle \lim_{x \rightarrow 0^+} \frac{1}{x^r} = +\infty $ &
 
 $ \displaystyle \lim_{x \rightarrow 0^-} \frac{1}{x^r} = \begin{cases}
 -\infty & \text{ if $r$ is odd } \\
 +\infty & \text{ if $r$ is even }
 \end{cases}
 $
\end{tabularx}


\subsubsection{Polynomials}
$ \displaystyle \lim_{x\rightarrow\pm\infty}P(x)=\lim_{x\rightarrow\pm\infty}a_nx^x=\pm\infty $

$ \displaystyle \lim_{x\rightarrow\pm\infty}\frac{P(x)}{Q(x)}=\lim_{x\rightarrow\pm\infty}\frac{a_nx^x}{b_mx^m}=\lim_{x\rightarrow\pm\infty}\frac{a_n}{b_m}x^{n-m}=
\begin{cases}
\infty & \text{if } n>m \\
\frac{a_n}{b_m} & \text{if } n=m \\
0 & \text{if } n<m \\
\end{cases} $

\subsubsection{Trigonometric functions}
\begin{tabularx}{\textwidth}{ X X X }

$ \displaystyle \lim_{x\rightarrow0}\frac{\sin(x)}{x}=1 $ &
$ \displaystyle \lim_{x\rightarrow0}\frac{\sin(\alpha x)}{\beta x}=\frac{\alpha}{\beta} $ &
$ \displaystyle  \lim_{x\rightarrow0}\frac{\arcsin(x)}{x}=1 $\\ [1.7ex]

$ \displaystyle \lim_{x\rightarrow0}\frac{1-\cos(x)}{x}=0 $ &
$ \displaystyle \lim_{x\rightarrow0}\frac{1-\cos(x)}{x^2}=\frac{1}{2} $ \\ [1.7ex]

$ \displaystyle \lim_{x\rightarrow0}\frac{\tan(x)}{x}=1 $ &
$ \displaystyle \lim_{x\rightarrow0}\frac{\arctan(x)}{x}=1 $ 

\end{tabularx}

\subsubsection{Logarithmic and exponential functions}
\begin{tabularx}{\textwidth}{ X X }
	
$ \displaystyle \lim_{x\rightarrow0}\frac{\ln(x+a)}{x}=a $ &
$ \displaystyle \lim_{x\rightarrow0}\frac{\log_a(x+1)}{x}=\frac{1}{\ln(a)} ~~~ (a>0) $ \\ [2ex]

$ \displaystyle \lim_{x\rightarrow+\infty}x^\alpha= \begin{cases}
+\infty & a>0 \\
0 & a<0
\end{cases} $ & 
$ \displaystyle \lim_{x\rightarrow0^+}x^\alpha= \begin{cases}
+\infty & a<0 \\
0 & a>0
\end{cases} $ \\ [2.2ex]

$ \displaystyle \lim_{x\rightarrow+\infty}a^x= \begin{cases}
+\infty & a>1 \\
0 & a<1
\end{cases} $ & 
$ \displaystyle \lim_{x\rightarrow-\infty}a^x= \begin{cases}
0 & a>1 \\
+\infty & a<1 
\end{cases} $ \\ [2.2ex]

$ \displaystyle \lim_{x\rightarrow+\infty}\log_ax= \begin{cases}
-\infty & a<1 \\
+\infty & a>1 
\end{cases} $ & 
$ \displaystyle \lim_{x\rightarrow 0^+}\log_ax= \begin{cases}
-\infty & a>1 \\
+\infty & a<1 
\end{cases} $ \\ [2.2ex]

$ \displaystyle\lim_{x\rightarrow0}\frac{a^x-1}{x}=ln(a) ~~ \forall a>0 $ &
 $ \displaystyle \lim_{x\rightarrow0}\frac{e^x-1}{x}=1 $ \\[2ex]

$ \displaystyle \lim_{x\rightarrow+\infty}\frac{x^\alpha}{a^x}=0  ~~~~ a>1 $ &
$ \displaystyle \lim_{x\rightarrow-\infty}\frac{e^x}{\left|x\right|^\alpha}=0 ~~~~ a>1 $ \\ [2ex]

$ \displaystyle \lim_{x\rightarrow+\infty}\frac{\ln(x)}{x^\alpha}=0 $ &
$ \displaystyle \lim_{x\rightarrow 0^+}\ln(x)x^\alpha=0 ~~~~ a>0 $ \\ [2ex]

$ \displaystyle \lim_{x\rightarrow+\infty}\frac{\ln(x)}{a^x}=0 ~~~ a>1$ \\ [2ex]

$ \displaystyle \lim_{x\rightarrow+\infty}\left(1+\frac{1}{x}\right)^x=e $ & 
$ \displaystyle \lim_{x\rightarrow+\infty}\left(1-\frac{1}{x}\right)^x=\frac{1}{e} $ \\ [2ex]

$ \displaystyle \lim_{x\rightarrow+\infty}\left(\frac{x}{x+k}\right)^x=\frac{1}{e^k} $ & 
$ \displaystyle \lim_{x\rightarrow+\infty}\left(\frac{x}{ \sqrt[x]{x!} }\right)=e $ \\ [2ex]

$ \displaystyle \lim_{x\rightarrow0}(1+x)^{1/x}={\rm e} $ &
$ \displaystyle \lim_{x\rightarrow+\infty}\left(1+\frac{n}{x}\right)^{mx}={\rm e}^{mn} $ \\ [2ex]

$ \displaystyle \lim_{x\rightarrow\infty}\frac{x^p}{a^x}=0~~\mbox{als }|a|>1 $ &
$ \displaystyle \lim_{x\rightarrow0}\frac{(1+x)^\alpha-1}{x}=\alpha ~~~ (a\in\mathbb{R}) $ \\ [2ex]

$ \displaystyle \lim_{x\rightarrow0}\left(a^{1/x}-1\right)=\ln(a) $ &
$ \displaystyle \lim_{x\rightarrow\infty}\sqrt[x]{x}=1 $ \\ [2ex]

\end{tabularx}

\section{Series}
\textbf{Definition:} In mathematics, a series is, roughly speaking, a description of the operation of adding infinitely many quantities, one after the other, to a given starting quantity.

\subsection{Convergence and divergence}
Absolute Convergence: If $\sum\left|s_n\right|$ is convergent.

Conditional Convergence: If $\sum s_n$ is convergent but not absolutely convergent.

If $\sum\limits_n|u_n|$ converges, $\sum\limits_n u_n$ also converges.

If $\lim\limits_{n\rightarrow\infty}u_n\neq0$ then $\sum\limits_n u_n$ is divergent.

\subsection{Positive Series}
Positive Series: If all the terms $ s^n $ are positive.

If $u_n>0~\forall n$ then $\sum\limits_n u_n$ is convergent if 
$\sum\limits_n\ln(u_n+1)$ is convergent.

Integral Test: If $ f(n) = s_n $, continuous, positive, decreasing: $\sum s_n$ converges $ \Leftrightarrow \int_{1}^{\infty}f(x)dx $ converges.

Comparison Test: $\sum a_n$ and $\sum b_n$ where $a_k<b_k ~ (\forall k\ge m) 
\begin{cases}
\text{If } \sum b_n \text{ converges, so does } \sum a_n \\
\text{If } \sum a_n \text{ diverges, so does } \sum b_n
\end{cases}
$

Limit Comparison Test: $\sum a_n$ and $\sum b_n$ such that $ \lim_{n\rightarrow\infty} \frac{a_n}{b_n}$ exists, $\sum a_n$ converges $\Leftrightarrow$ $\sum b_n$ converges.

Ratio Test: if $\lim_{n\rightarrow \infty}\left|\frac{s_{n+1}}{s_n}\right| = \begin{cases}
<1 \text{ then it is absolutely convergent} \\
1 \text{ then no conclusion} \\
>1 \text{ or } +\infty \text{ then it diverges}
\end{cases}
$

Root Test: if  $\lim_{n\rightarrow \infty} \sqrt[n]{\left|s_n\right|} = \begin{cases}
<1 \text{ then it is absolutely convergent} \\
1 \text{ then no conclusion} \\
>1 \text{ or } +\infty \text{ then it diverges}
\end{cases}
$

\subsection{Alternating Series}
Alternating Series: $ \sum(-1)^{n+1}a_n=a_1-a_2+a_3-a_4+a_5-... $

Leibniz Test: An alternating series of which the absolute values of the terms drop
monotonously to 0 is convergent.

\subsection{Common Series}
\subsubsection{Basic Series}

\begin{tabular}{ l l }
$ \displaystyle \sum_{k=1}^n k = \frac{n(n+1)}{2} $ &
$ \displaystyle \sum_{k=1}^n (2k-1) = n^2 $ \\ [1.5em]
$ \displaystyle \sum_{k=1}^n k^2 = \frac{n(n+1)(2n+1)}{6} $ & 
$ \displaystyle \sum_{k=1}^n k^3 = \frac{n^2(n+1)^2}{4} $
\end{tabular}

\subsubsection{Harmonic Series}
\[
\sum_{n=1}^\infty \frac{1}{n^p} ~ \begin{cases}
									\text{is convergent if } p>1 \\
									\text{is divergent if } p\leq1
									\end{cases}
\]

\subsubsection{Geometric Series}
\[
\sum_{0}^{n}q^k=1+q+q^2+...+q^n= \begin{cases}
									n+1 & q=1 \\
									\frac{1-q^{k+1}}{1-q} & q\ne1
								 \end{cases}
~ \text{ then } ~
\sum_{0}^{+\infty}q^k= \begin{cases}
\frac{1}{1-q} & \left|q\right|<1 \\
+\infty & q\ge1 \\
\text{no conclusion} & q\le-1
\end{cases}
\]

\subsubsection{Mengoli Series}
\[
s_n=\sum_{k=2}^{n}\frac{1}{(k-1)k}=1-\frac{1}{n} ~~ \text{ then } ~~  \lim_{n\rightarrow\infty}s_n=1
\]

\section{Landau Symbols}
\subsection{Big-O notation}
The Big-O provides a function that is at most the same order as that of a given function.

If $\exists M>0$ such that $\displaystyle \lim_{x\rightarrow c} \left| \frac{f(x)}{g(x)} \right| < M $ then we say "as $x\rightarrow c, f(x)=O(g(x))$"

\subsection{Little-O notation}
The little-o provides a function that is of lower order of magnitude than a given function.

If $\displaystyle \lim_{x\rightarrow c} \frac{f(x)}{g(x)} = 0 $ then we say "as $x\rightarrow c, f(x)=o(g(x))$"



\section{Asymptotes}
In analytic geometry, an asymptote of a curve is a line such that the distance between the curve and the line approaches zero as one or both of the x or y coordinates tends to infinity.

\subsection{Vertical asymptotes}
The line $x = a$ is a vertical asymptote of the graph of the function $y = f(x)$ if:

 $\displaystyle \lim_{x\rightarrow a^-} f(x) = \pm \infty $ or 
 $\displaystyle \lim_{x\rightarrow a^+} f(x) = \pm \infty $


\subsection{Horizontal asymptotes}
The horizontal line $y = c$ is a horizontal asymptote of the function $y = f(x)$ if:

 $\displaystyle \lim_{x\rightarrow -\infty} f(x) = c $ or 
 $\displaystyle \lim_{x\rightarrow + \infty} f(x) = c $
 
\subsection{Oblique asymptotes}
A function $f(x)$ is asymptotic to the straight line $y = mx + n (m \ne 0)$ if:

$\displaystyle \lim_{x\rightarrow +\infty} \left[ f(x) - (mx+n) \right] = 0 $ or 
$\displaystyle \lim_{x\rightarrow -\infty} \left[ f(x) - (mx+n) \right] = 0 $

Using the Landau symbols, the formula becomes:

$\displaystyle  f(x) = mx + n + 0(1), x\rightarrow +\infty$ 

From the last one, it is easy to find that:

$\displaystyle m=\lim_{x\rightarrow +\infty} \frac{f(x)}{x} $  and $\displaystyle q=\lim_{x\rightarrow +\infty} (f(x) - mx) $

\section{Derivatives}
The derivative of a function is the ratio of the difference of function value $ f(x) $ at points $ x+\Delta x $ and $ x $ with $ \Delta x $, when $ \Delta x $ is infinitesimally small. The derivative is the function slope or slope of the tangent line at point x.

\[
f^\prime(x) = \lim_{\Delta x \rightarrow 0} \frac{ f( x + \Delta x ) - f(x) }{\Delta x}
\]

\subsection{Derivatives rules}
\begin{tabular}{ l l }
Sum rule:      & $ \displaystyle (\alpha f(x) \pm \beta g(x) )^\prime = \alpha f^\prime(x) \pm \beta g^\prime(x) $  \\
Product rule:  & $ \displaystyle (f(x) \cdot g(x) )^\prime = f^\prime(x) g(x) + f(x) g^\prime(x) $ \\
Quotient rule: & $ \displaystyle \left( \frac{f(x)}{g(x)} \right)^\prime = \frac{ f^\prime(x) g(x) - f(x) g^\prime(x) }{ g^2(x) } $ \\
Chain rule:    & $ \displaystyle f\left( g(x) \right) ^\prime = f^\prime\left( g(x) \right) \cdot g^\prime(x) $
\end{tabular}

\subsection{Derivative test}

\subsubsection{Second derivative test}
If the function $f$ is twice differentiable at a critical point $x_0$ (i.e. $f^\prime(x_0) = 0$), then:
\begin{itemize}
\item If $ f^{\prime \prime }(x_0)<0 $ then $f$  has a local maximum at $x_0$.
\item If $ f^{\prime \prime }(x_0)>0 $ then $f$  has a local minimum at $x_0$.
\item If $ f^{\prime \prime }(x_0)=0 $ the test is inconclusive.
\end{itemize}

\subsubsection{Concavity test}
A twice-differentiable function $f$ is concave up if $ f^{\prime \prime }(x_0)>0 $ and concave down if $ f^{\prime \prime }(x_0)<0 $.

\subsubsection{Higher order derivative test}
Let $f$ be a real-valued function $n$-times ($n\ge2$) differentiable at a point $x_0$ with
\[
f^\prime(x_0) = ... = f^{(n)}(x_0) = 0 ~~~, ~~~ f^{(n+1)}(x_0) \ne 0
\]
then
\begin{itemize}
	\item If $n$ is odd and  $ f^{(n+1)}(x_0)<0 $ then $f$ has a local maximum at $x_0$.
	\item If $n$ is odd and  $ f^{(n+1)}(x_0)>0 $ then $f$ has a local minimum at $x_0$.
	\item If $n$ is even and  $ f^{(n+1)}(x_0)<0 $ then $x_0$ is a strictly decreasing point of inflection.
	\item If $n$ is even and  $ f^{(n+1)}(x_0)>0 $ then $x_0$ is a strictly increasing point of inflection.
\end{itemize}

\medskip
In addition, when $x_0$ is not a critical point, we have:

Let $f$ be a real-valued function $n$-times ($n\ge3$) differentiable at a point $x_0$ with
\[
f^\prime(x_0)\ne0 ~~~, ~~~ f^{\prime\prime}(x_0) = ... = f^{(n)}(x_0) = 0 ~~~, ~~~ f^{(n+1)}(x_0) \ne 0
\]
then
\begin{itemize}
	\item If $n$ is odd then $x_0$ is not an inflection point.
	\item If $n$ is even and  $ f^{(n+1)}(x_0)<0 $ then $x_0$ is a strictly decreasing point of inflection.
	\item If $n$ is even and  $ f^{(n+1)}(x_0)>0 $ then $x_0$ is a strictly increasing point of inflection.
\end{itemize}



\subsection{Common derivatives}
\subsubsection{Basic Derivatives}
$ \displaystyle f(x)=\alpha ~ \Rightarrow ~ f^\prime(x)=0 $

$ \displaystyle f(x)=x ~ \Rightarrow ~ f^\prime(x)=1 $

$ \displaystyle f(x)=x^n ~ \Rightarrow ~ f^\prime(x)=n x^{n-1} $

\subsubsection{Trigonometric Derivatives}
\begin{tabular}{ l l }
$ \displaystyle f(x)=\sin(x) ~ \Rightarrow ~ f^\prime(x)=\cos(x) $ & 
$ \displaystyle f(x)=\arcsin(x) ~ \Rightarrow ~ f^\prime(x)= \frac{1}{ \sqrt{1-x^2} } $ \\ [2ex]
$ \displaystyle f(x)=\cos(x) ~ \Rightarrow ~ f^\prime(x)=-\sin(x) $ &
$ \displaystyle f(x)=\arccos(x) ~ \Rightarrow ~ f^\prime(x)= -\frac{1}{ \sqrt{1-x^2} } $ \\ [2ex]
$ \displaystyle f(x)=\tan(x) ~ \Rightarrow ~ f^\prime(x)=\sec^2(x)= \frac{1}{\cos^2(x)} $ & 
$ \displaystyle f(x)=\arctan(x) ~ \Rightarrow ~ f^\prime(x)= \frac{1}{ 1+x^2 } $ \\ [2ex]
$ \displaystyle f(x)=\sec(x) ~ \Rightarrow ~ f^\prime(x)=\sec(x)\tan(x) $ \\ [2ex]
$ \displaystyle f(x)=\cot(x) ~ \Rightarrow ~ f^\prime(x)=-\csc^2(x) $ \\ [2ex]
$ \displaystyle f(x)=\csc(x) ~ \Rightarrow ~ f^\prime(x)=-\csc(x)\cot(x) $
\end{tabular}


\subsubsection{Exponential Derivatives}
\begin{tabular}{ l l }
$ \displaystyle  f(x)=a^x ~ \Rightarrow ~ f^\prime(x)=\ln(a)a^x $ & 
$ \displaystyle  f(x)=e^x ~ \Rightarrow ~ f^\prime(x)=e^x $ \\ 
$ \displaystyle  f(x)=a^{g(x)} ~ \Rightarrow ~ f^\prime(x)=\ln(a)a^{g(x)}g^\prime(x) $ & 
$ \displaystyle  f(x)=e^{g(x)} ~ \Rightarrow ~ f^\prime(x)=e^{g(x)}g^\prime(x) $ 
\end{tabular}


\subsubsection{Logarithm Derivatives}
\begin{tabular}{ l l }
$ \displaystyle  f(x)=\log_a(x) ~ \Rightarrow ~ f^\prime(x)= \frac{1}{ \ln(a)x } $ & 
$ \displaystyle  f(x)=\ln(x) ~ \Rightarrow ~ f^\prime(x)= \frac{1}{ x } $ \\ [1.5ex] 
$ \displaystyle  f(x)=\log_a(g(x)) ~ \Rightarrow ~ f^\prime(x)= \frac{g^\prime(x)}{ \ln(a)g(x) } $ & 
$ \displaystyle  f(x)=\ln(g(x)) ~ \Rightarrow ~ f^\prime(x)= \frac{g^\prime(x)}{ g(x) } $ 
\end{tabular}


\section{Taylor Series}
A Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point.

\textbf{Taylor's Theorem:}
Let $k \ge 1$ be an integer and let the function $f : \mathbb{R} \rightarrow \mathbb{R}$ be $k$ times differentiable at the point $a \in R$. Then there exists a function $h_k : \mathbb{R} \rightarrow \mathbb{R}$ such that
\[
f(x) = f(a) + f^{\prime}(a)(x-a) + \frac{f^{\prime\prime}(a)}{2!}(x-a)^2 + ... + \frac{f^{k}(a)}{k!}(x-a)^k + h_k ( x ) (x-a)^k
\]
and $\lim_{x \rightarrow a} h_k ( x ) = 0$. This is called the Peano form of the remainder.


The Taylor series of a real or complex-valued function $f(x)$ that is infinitely differentiable at a real or complex number $a$ is the power series:
\[
\sum_{n=0}^{+\infty}\frac{f^{(n)}(a)}{n!}(x-a)^n
\]

\textbf{Maclaurin series:}
When $a = 0$, the series is also called a Maclaurin series.

\textbf{Even/Odd functions and Maclaurin Polynomials:}
\begin{itemize}
	\item If $f$ be an even function, then each of $f$'s Maclaurin polynomials contains only even powers;
	\item If $f$ be an odd function, then each of $f$'s Maclaurin polynomials contains only odd powers.
\end{itemize}

\subsection{Maclaurin series of common functions}

\subsubsection{Basic functions}
$\displaystyle \sqrt{1+x} = 1 + \frac{1}{2}x - \frac{1}{8}x^2 + \frac{1}{16}x^3 + o(x^3)$ 

\subsubsection{Exponential functions}
$\displaystyle e^x = \sum_{n=0}^{+\infty}\frac{x^n}{n!} = 1 + x + \frac{x^2}{2!} + ... + \frac{x^n}{n!} + o(x^n) $ \ \ \ \ It converges for all $x$

\subsubsection{Logarithm functions}
$\displaystyle \ln(1+x) = \sum_{n=1}^{+\infty}(-1)^{n+1}\frac{x^n}{n} = x - \frac{x^2}{2} + ... + (-1)^{n+1}\frac{x^n}{n} + o(x^n) $ \ \ \ It conv. for $\left|x\right|<1$

\subsubsection{Geometric Series}
$\displaystyle \frac{1}{1+x} = \sum_{n=0}^{+\infty}(-1)^n x^n = 1 - x + x^2 ... + (-1)^n x^n + o(x^n)$ 

$\displaystyle \frac{1}{1-x} = \sum_{n=0}^{+\infty}x^n $ \ \ \ \ It converges for $\left|x\right|<1$.

$\displaystyle \frac{1}{(1-x)^2} = \sum_{n=1}^{+\infty}nx^{n-1} $ \ \ \ \ It converges for $\left|x\right|<1$.

$\displaystyle \frac{1}{(1-x)^3} = \sum_{n=2}^{+\infty} \frac{(n-1)n}{2} x^{n-2} $ \ \ \ \ It converges for $\left|x\right|<1$.

\subsubsection{Binomial Series}
$\displaystyle (1+x)^\alpha = \sum_{n=0}^{+\infty} \binom{\alpha}{n} x^n $ \ \ It converges for $\left|x\right|<1$ for any real or complex number $\alpha$.

\subsubsection{Trigonometric functions}
\begin{align*}
	\sin(x) &= \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n+1)!}x^{2n+1} &= x - \frac{x^3}{6} + \frac{x^5}{120} - ... ~~~~~~ & \forall x & \\
	\cos(x) &= \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n)!}x^{2n} &= 1 - \frac{x^2}{2} + \frac{x^4}{24} - ... ~~~~~~ & \forall x &\\
	\tan(x) &= \sum_{n=1}^{\infty} \frac{B_{2n}(-4)^n (1-4^n) }{(2n)!}x^{2n-1} &= x + \frac{x^3}{3} + \frac{2x^5}{15} + ... ~~~~~~ & \text{for } |x|<\frac{\pi}{2} &\\
\end{align*}

\subsubsection{Hyperbolic functions}
\begin{align*}
\sinh(x) &= \sum_{n=0}^{\infty} \frac{x^{2n+1}}{(2n+1)!} &= x + \frac{x^3}{3!} + \frac{x^5}{5!} + ... ~~~~~~ & \forall x & \\
\cosh(x) &= \sum_{n=0}^{\infty} \frac{x^{2n}}{(2n)!} &= 1 + \frac{x^2}{2!} + \frac{x^4}{4!} + ... ~~~~~~ & \forall x & \\
\tanh(x) &= \sum_{n=1}^{\infty} \frac{B_{2n} 4^n (4^n-1) }{(2n)!}x^{2n-1} &= x - \frac{x^3}{3} + \frac{2x^5}{15} - \frac{17x^7}{315} + ... ~~~~~~ & \text{for } |x|<\frac{\pi}{2} & \\
\end{align*}

\section{Integral Calculus}
\textbf{Definition:} In mathematics, Integral calculus is a subfield of calculus in which the notion of an integral, its properties and methods of calculation are studied. It concerns accumulation of quantities and the areas under and between curves.

\subsection{Integration rules}
\begin{tabular}{ l l }
	Multiplication by a constant:      & $ \displaystyle \int c f(x)dx = c \int f(x)dx  $  \\
	Sum rule:      & $ \displaystyle \int (f(x) + g(x)) dx = \int f(x)dx + \int g(x)dx $  \\
	Integration by parts:   & $ \displaystyle \int f(x)g^\prime(x)dx = f(x)g(x) + \int f^\prime(x)g(x)dx  $  \\
	Substitution rule: & $ \displaystyle \int f(g(x))g^\prime(x)dx = \int f(y)dy ~~,~~ y=g(x) $  \\
	\end{tabular}


\subsection{Integration of Rational Functions}
A rational function $ \frac{P(x)}{Q(x)}$, where $P(x)$ and $Q(x)$ are both polynomials, can be integrated in four steps:
\begin{enumerate}
\item Reduce the fraction if it is improper (i.e. degree of $P(x)$ is greater than degree of $Q(x)$);
\item Factor $Q(x)$ into linear and/or quadratic (irreducible) factors;
\item Decompose the fraction into a sum of partial fractions;
\item Calculate integrals of each partial fraction.
\end{enumerate}


\subsubsection{Step 1. Reducing an Improper Fraction}
If the fraction is improper, divide the $P(x)$ by $Q(x)$ to obtain
\[\frac{P(x)}{Q(x)}=F(x)+\frac{R(x)}{Q(x)}\]
where $\frac{R(x)}{Q(x)}$ is a proper fraction.

\subsubsection{Step 2. Factoring Q(x) into Linear and/or Quadratic Factors}
Write the denominator $Q(x)$ as
\[
Q(x) = (x-a)^\alpha \cdot\cdot\cdot (x-b)^\beta(x^2+px+q)^\mu \cdot\cdot\cdot (x^2+rx+s)^\nu
\]
where quadratic functions are irreducible, i.e. do not have real roots.

\subsubsection{Step 3. Decomposing the Rational Fraction into a Sum of Partial Fractions}
Write the function as follows:
\[\frac{R(x)}{Q(x)}= \frac{A_1}{(x-a)^{\alpha}} + \frac{A_2}{(x-a)^{\alpha-1}} + ... + \frac{A_\alpha}{(x-a)} + ... + \frac{M_1 x + N_1}{(x^2+rx+s)^{\nu}} + ... + \frac{M_\nu x + N_\nu}{(x^2+rx+s)}\]
Then equate the coefficients of equal powers of $x$ by multiplying both sides of the latter expression by $Q(x)$ and write the system of linear equations in $A_i,B_i,M_i,N_i,...$ .

\subsubsection{Step 4. Integrating partial fractions}
Use the following formulas to evaluate integrals of partial fractions with linear and quadratic denominators:

\begin{enumerate}
	\item $\displaystyle \int \frac{A}{x-a} dx = A \ln \left| x-a \right| +C$

	\item $\displaystyle \int \frac{A}{(x-a)^k} dx = \frac{A}{(1-k)(x-a)^{k-1}} +C$
	
	\item $\displaystyle \int \frac{1}{(x-b)^2+c^2} dx = \frac{1}{c} \arctan \frac{x-b}{c} + C$
	
	\item $\displaystyle \int \frac{2x-2b}{(x-b)^2+c^2} dx = \ln \left| (x-b)^2 + c^2 \right| +C$
	
	\item $\displaystyle \int \frac{1}{(x^2+bx+c)^n} dx = !!TODO!!$
	
\end{enumerate}


\subsection{Integration of Common Functions - TODO}

\section{Differential calculus}
\textbf{Definition:} In mathematics, differential calculus is a subfield of calculus concerned with the study of the rates at which quantities change.

